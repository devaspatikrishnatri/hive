PREHOOK: query: CREATE TABLE t1 (a int, b varchar(256), c decimal(10,2), d int) STORED AS orc TBLPROPERTIES ('transactional'='true')
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@t1
POSTHOOK: query: CREATE TABLE t1 (a int, b varchar(256), c decimal(10,2), d int) STORED AS orc TBLPROPERTIES ('transactional'='true')
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@t1
PREHOOK: query: INSERT INTO t1 VALUES
 (NULL, 'row with NULL GBY key', 100.77, 7),
 (NULL, 'row with NULL GBY key', 100.77, 8),
 (NULL, 'row with NULL GBY key will be inserted', 100.77, 7),
 (1, 'row with NULL aggregated value', 100.77, NULL),
 (1, 'row with NULL aggregated value will be inserted', 100.77, NULL),
 (2, 'average row', 100, 10),
 (2, 'average row', 100, 11),
 (3, 'average row will be inserted', 100, 20),
 (4, 'row with NULL aggregated value + non null value will be inserted', 100.77, NULL),
 (5, 'average row + null value will be inserted', 100, 11),
 (NULL, NULL, 200, 100)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@t1
POSTHOOK: query: INSERT INTO t1 VALUES
 (NULL, 'row with NULL GBY key', 100.77, 7),
 (NULL, 'row with NULL GBY key', 100.77, 8),
 (NULL, 'row with NULL GBY key will be inserted', 100.77, 7),
 (1, 'row with NULL aggregated value', 100.77, NULL),
 (1, 'row with NULL aggregated value will be inserted', 100.77, NULL),
 (2, 'average row', 100, 10),
 (2, 'average row', 100, 11),
 (3, 'average row will be inserted', 100, 20),
 (4, 'row with NULL aggregated value + non null value will be inserted', 100.77, NULL),
 (5, 'average row + null value will be inserted', 100, 11),
 (NULL, NULL, 200, 100)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@t1
POSTHOOK: Lineage: t1.a SCRIPT []
POSTHOOK: Lineage: t1.b SCRIPT []
POSTHOOK: Lineage: t1.c SCRIPT []
POSTHOOK: Lineage: t1.d SCRIPT []
_col0	_col1	_col2	_col3
PREHOOK: query: CREATE MATERIALIZED VIEW mat1 TBLPROPERTIES ('transactional'='true') AS
  SELECT a, b, sum(d), min(d), max(d)
  FROM t1
  WHERE c > 10.0
  GROUP BY a, b
PREHOOK: type: CREATE_MATERIALIZED_VIEW
PREHOOK: Input: default@t1
PREHOOK: Output: database:default
PREHOOK: Output: default@mat1
POSTHOOK: query: CREATE MATERIALIZED VIEW mat1 TBLPROPERTIES ('transactional'='true') AS
  SELECT a, b, sum(d), min(d), max(d)
  FROM t1
  WHERE c > 10.0
  GROUP BY a, b
POSTHOOK: type: CREATE_MATERIALIZED_VIEW
POSTHOOK: Input: default@t1
POSTHOOK: Output: database:default
POSTHOOK: Output: default@mat1
POSTHOOK: Lineage: mat1._c2 EXPRESSION [(t1)t1.FieldSchema(name:d, type:int, comment:null), ]
POSTHOOK: Lineage: mat1._c3 EXPRESSION [(t1)t1.FieldSchema(name:d, type:int, comment:null), ]
POSTHOOK: Lineage: mat1._c4 EXPRESSION [(t1)t1.FieldSchema(name:d, type:int, comment:null), ]
POSTHOOK: Lineage: mat1.a SIMPLE [(t1)t1.FieldSchema(name:a, type:int, comment:null), ]
POSTHOOK: Lineage: mat1.b SIMPLE [(t1)t1.FieldSchema(name:b, type:varchar(256), comment:null), ]
a	b	_c2	_c3	_c4
PREHOOK: query: INSERT INTO t1 VALUES
 (NULL, 'new row with NULL GBY key', 100.77, 35),
 (NULL, 'new row with NULL GBY key', 100.77, 36),
 (NULL, 'row with NULL GBY key will be inserted', 100.77, 7),
 (1, 'new row with NULL aggregated value', 100.77, NULL),
 (1, 'row with NULL aggregated value will be inserted', 100.77, NULL),
 (2, 'new average row', 100, 50),
 (2, 'new average row', 100, 51),
 (3, 'average row will be inserted', 100, 20),
 (4, 'row with NULL aggregated value + non null value will be inserted', 100.77, 100),
 (5, 'average row + null value will be inserted', 100, NULL),
 (NULL, NULL, 200, 100)
PREHOOK: type: QUERY
PREHOOK: Input: _dummy_database@_dummy_table
PREHOOK: Output: default@t1
POSTHOOK: query: INSERT INTO t1 VALUES
 (NULL, 'new row with NULL GBY key', 100.77, 35),
 (NULL, 'new row with NULL GBY key', 100.77, 36),
 (NULL, 'row with NULL GBY key will be inserted', 100.77, 7),
 (1, 'new row with NULL aggregated value', 100.77, NULL),
 (1, 'row with NULL aggregated value will be inserted', 100.77, NULL),
 (2, 'new average row', 100, 50),
 (2, 'new average row', 100, 51),
 (3, 'average row will be inserted', 100, 20),
 (4, 'row with NULL aggregated value + non null value will be inserted', 100.77, 100),
 (5, 'average row + null value will be inserted', 100, NULL),
 (NULL, NULL, 200, 100)
POSTHOOK: type: QUERY
POSTHOOK: Input: _dummy_database@_dummy_table
POSTHOOK: Output: default@t1
POSTHOOK: Lineage: t1.a SCRIPT []
POSTHOOK: Lineage: t1.b SCRIPT []
POSTHOOK: Lineage: t1.c SCRIPT []
POSTHOOK: Lineage: t1.d SCRIPT []
_col0	_col1	_col2	_col3
PREHOOK: query: explain cbo
ALTER MATERIALIZED VIEW mat1 REBUILD
PREHOOK: type: QUERY
PREHOOK: Input: default@mat1
PREHOOK: Input: default@t1
PREHOOK: Output: default@mat1
PREHOOK: Output: default@mat1
POSTHOOK: query: explain cbo
ALTER MATERIALIZED VIEW mat1 REBUILD
POSTHOOK: type: QUERY
POSTHOOK: Input: default@mat1
POSTHOOK: Input: default@t1
POSTHOOK: Output: default@mat1
POSTHOOK: Output: default@mat1
Explain
CBO PLAN:
HiveProject($f0=[$6], $f1=[$7], $f2=[CASE(IS NULL($2), $8, IS NULL($8), $2, +($8, $2))], $f3=[CASE(IS NULL($3), $9, IS NULL($9), $3, CASE(<($9, $3), $9, $3))], $f4=[CASE(IS NULL($4), $10, IS NULL($10), $4, CASE(>($10, $4), $10, $4))])
  HiveFilter(condition=[OR($5, IS NULL($5))])
    HiveJoin(condition=[AND(IS NOT DISTINCT FROM($0, $6), IS NOT DISTINCT FROM($1, $7))], joinType=[right], algorithm=[none], cost=[not available])
      HiveProject(a=[$0], b=[$1], _c2=[$2], _c3=[$3], _c4=[$4], $f5=[true])
        HiveTableScan(table=[[default, mat1]], table:alias=[default.mat1])
      HiveProject(a=[$0], b=[$1], $f2=[$2], $f3=[$3], $f4=[$4])
        HiveAggregate(group=[{0, 1}], agg#0=[sum($3)], agg#1=[min($3)], agg#2=[max($3)])
          HiveFilter(condition=[AND(>($2, 10:DECIMAL(2, 0)), <(1, $6.writeid))])
            HiveTableScan(table=[[default, t1]], table:alias=[t1])

PREHOOK: query: explain
ALTER MATERIALIZED VIEW mat1 REBUILD
PREHOOK: type: QUERY
PREHOOK: Input: default@mat1
PREHOOK: Input: default@t1
PREHOOK: Output: default@mat1
PREHOOK: Output: default@mat1
POSTHOOK: query: explain
ALTER MATERIALIZED VIEW mat1 REBUILD
POSTHOOK: type: QUERY
POSTHOOK: Input: default@mat1
POSTHOOK: Input: default@t1
POSTHOOK: Output: default@mat1
POSTHOOK: Output: default@mat1
Explain
STAGE DEPENDENCIES:
  Stage-7 is a root stage
  Stage-2 depends on stages: Stage-7
  Stage-0 depends on stages: Stage-2
  Stage-3 depends on stages: Stage-0
  Stage-8 depends on stages: Stage-3, Stage-6
  Stage-4 depends on stages: Stage-2
  Stage-6 depends on stages: Stage-1, Stage-4
  Stage-5 depends on stages: Stage-2
  Stage-1 depends on stages: Stage-5

STAGE PLANS:
  Stage: Stage-7
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: t1
            filterExpr: ((c > 10) and (ROW__ID.writeid > 1L)) (type: boolean)
            Statistics: Num rows: 22 Data size: 25640 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: ((c > 10) and (ROW__ID.writeid > 1L)) (type: boolean)
              Statistics: Num rows: 2 Data size: 2330 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: a (type: int), b (type: varchar(256)), d (type: int)
                outputColumnNames: a, b, d
                Statistics: Num rows: 2 Data size: 2330 Basic stats: COMPLETE Column stats: NONE
                Group By Operator
                  aggregations: sum(d), min(d), max(d)
                  keys: a (type: int), b (type: varchar(256))
                  minReductionHashAggr: 0.99
                  mode: hash
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4
                  Statistics: Num rows: 2 Data size: 2330 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: int), _col1 (type: varchar(256))
                    null sort order: aa
                    sort order: ++
                    Map-reduce partition columns: _col0 (type: int), _col1 (type: varchar(256))
                    Statistics: Num rows: 2 Data size: 2330 Basic stats: COMPLETE Column stats: NONE
                    value expressions: _col2 (type: bigint), _col3 (type: int), _col4 (type: int)
      Reduce Operator Tree:
        Group By Operator
          aggregations: sum(VALUE._col0), min(VALUE._col1), max(VALUE._col2)
          keys: KEY._col0 (type: int), KEY._col1 (type: varchar(256))
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3, _col4
          Statistics: Num rows: 1 Data size: 1165 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-2
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: default.mat1
            Statistics: Num rows: 9 Data size: 12350 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: a (type: int), b (type: varchar(256)), _c2 (type: bigint), _c3 (type: int), _c4 (type: int), true (type: boolean), ROW__ID (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
              Statistics: Num rows: 9 Data size: 12350 Basic stats: COMPLETE Column stats: NONE
              Reduce Output Operator
                key expressions: _col0 (type: int), _col1 (type: varchar(256))
                null sort order: aa
                sort order: ++
                Map-reduce partition columns: _col0 (type: int), _col1 (type: varchar(256))
                Statistics: Num rows: 9 Data size: 12350 Basic stats: COMPLETE Column stats: NONE
                value expressions: _col2 (type: bigint), _col3 (type: int), _col4 (type: int), _col5 (type: boolean), _col6 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
          TableScan
            Reduce Output Operator
              key expressions: _col0 (type: int), _col1 (type: varchar(256))
              null sort order: aa
              sort order: ++
              Map-reduce partition columns: _col0 (type: int), _col1 (type: varchar(256))
              Statistics: Num rows: 1 Data size: 1165 Basic stats: COMPLETE Column stats: NONE
              value expressions: _col2 (type: bigint), _col3 (type: int), _col4 (type: int)
      Reduce Operator Tree:
        Join Operator
          condition map:
               Right Outer Join 0 to 1
          keys:
            0 _col0 (type: int), _col1 (type: varchar(256))
            1 _col0 (type: int), _col1 (type: varchar(256))
          nullSafes: [true, true]
          outputColumnNames: _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11
          Statistics: Num rows: 9 Data size: 13585 Basic stats: COMPLETE Column stats: NONE
          Filter Operator
            predicate: _col5 is null (type: boolean)
            Statistics: Num rows: 4 Data size: 6037 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: _col7 (type: int), _col8 (type: varchar(256)), CASE WHEN (_col2 is null) THEN (_col9) WHEN (_col9 is null) THEN (_col2) ELSE ((_col9 + _col2)) END (type: bigint), CASE WHEN (_col3 is null) THEN (_col10) WHEN (_col10 is null) THEN (_col3) ELSE (CASE WHEN ((_col10 < _col3)) THEN (_col10) ELSE (_col3) END) END (type: int), CASE WHEN (_col4 is null) THEN (_col11) WHEN (_col11 is null) THEN (_col4) ELSE (CASE WHEN ((_col11 > _col4)) THEN (_col11) ELSE (_col4) END) END (type: int)
              outputColumnNames: _col0, _col1, _col2, _col3, _col4
              Statistics: Num rows: 4 Data size: 6037 Basic stats: COMPLETE Column stats: NONE
              File Output Operator
                compressed: false
                Statistics: Num rows: 4 Data size: 6037 Basic stats: COMPLETE Column stats: NONE
                table:
                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                    name: default.mat1
                Write Type: INSERT
              Select Operator
                expressions: _col0 (type: int), _col1 (type: varchar(256)), _col2 (type: bigint), _col3 (type: int), _col4 (type: int)
                outputColumnNames: a, b, _c2, _c3, _c4
                Statistics: Num rows: 4 Data size: 6037 Basic stats: COMPLETE Column stats: NONE
                Group By Operator
                  aggregations: compute_stats(a, 'hll'), compute_stats(b, 'hll'), compute_stats(_c2, 'hll'), compute_stats(_c3, 'hll'), compute_stats(_c4, 'hll')
                  minReductionHashAggr: 0.99
                  mode: hash
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4
                  Statistics: Num rows: 1 Data size: 2136 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
          Filter Operator
            predicate: _col5 (type: boolean)
            Statistics: Num rows: 4 Data size: 6037 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: _col6 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), _col7 (type: int), _col8 (type: varchar(256)), CASE WHEN (_col2 is null) THEN (_col9) WHEN (_col9 is null) THEN (_col2) ELSE ((_col9 + _col2)) END (type: bigint), CASE WHEN (_col3 is null) THEN (_col10) WHEN (_col10 is null) THEN (_col3) ELSE (CASE WHEN ((_col10 < _col3)) THEN (_col10) ELSE (_col3) END) END (type: int), CASE WHEN (_col4 is null) THEN (_col11) WHEN (_col11 is null) THEN (_col4) ELSE (CASE WHEN ((_col11 > _col4)) THEN (_col11) ELSE (_col4) END) END (type: int)
              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
              Statistics: Num rows: 4 Data size: 6037 Basic stats: COMPLETE Column stats: NONE
              File Output Operator
                compressed: false
                table:
                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                    serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe

  Stage: Stage-0
    Move Operator
      tables:
          replace: false
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.mat1
          Write Type: INSERT

  Stage: Stage-3
    Stats Work
      Basic Stats Work:

  Stage: Stage-8
    Materialized View Update
      name: default.mat1
      update creation metadata: true

  Stage: Stage-4
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              null sort order: 
              sort order: 
              Statistics: Num rows: 1 Data size: 2136 Basic stats: COMPLETE Column stats: NONE
              value expressions: _col0 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:binary>), _col1 (type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:binary>), _col2 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:binary>), _col3 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:binary>), _col4 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:binary>)
      Reduce Operator Tree:
        Group By Operator
          aggregations: compute_stats(VALUE._col0), compute_stats(VALUE._col1), compute_stats(VALUE._col2), compute_stats(VALUE._col3), compute_stats(VALUE._col4)
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2, _col3, _col4
          Statistics: Num rows: 1 Data size: 2200 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 2200 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-6
    Stats Work
      Basic Stats Work:
      Column Stats Desc:
          Columns: a, b, _c2, _c3, _c4
          Column Types: int, varchar(256), bigint, int, int
          Table: default.mat1

  Stage: Stage-5
    Map Reduce
      Map Operator Tree:
          TableScan
            Reduce Output Operator
              key expressions: _col0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>)
              null sort order: a
              sort order: +
              Map-reduce partition columns: UDFToInteger(_col0) (type: int)
              Statistics: Num rows: 4 Data size: 6037 Basic stats: COMPLETE Column stats: NONE
              value expressions: _col1 (type: int), _col2 (type: varchar(256)), _col3 (type: bigint), _col4 (type: int), _col5 (type: int)
      Reduce Operator Tree:
        Select Operator
          expressions: KEY.reducesinkkey0 (type: struct<writeid:bigint,bucketid:int,rowid:bigint>), VALUE._col0 (type: int), VALUE._col1 (type: varchar(256)), VALUE._col2 (type: bigint), VALUE._col3 (type: int), VALUE._col4 (type: int)
          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
          Statistics: Num rows: 4 Data size: 6037 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 4 Data size: 6037 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
                serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                name: default.mat1
            Write Type: UPDATE

  Stage: Stage-1
    Move Operator
      tables:
          replace: false
          table:
              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
              name: default.mat1
          Write Type: UPDATE

PREHOOK: query: ALTER MATERIALIZED VIEW mat1 REBUILD
PREHOOK: type: QUERY
PREHOOK: Input: default@mat1
PREHOOK: Input: default@t1
PREHOOK: Output: default@mat1
PREHOOK: Output: default@mat1
POSTHOOK: query: ALTER MATERIALIZED VIEW mat1 REBUILD
POSTHOOK: type: QUERY
POSTHOOK: Input: default@mat1
POSTHOOK: Input: default@t1
POSTHOOK: Output: default@mat1
POSTHOOK: Output: default@mat1
POSTHOOK: Lineage: mat1._c2 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c2, type:bigint, comment:null), (t1)t1.FieldSchema(name:d, type:int, comment:null), ]
POSTHOOK: Lineage: mat1._c3 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c3, type:int, comment:null), (t1)t1.FieldSchema(name:d, type:int, comment:null), ]
POSTHOOK: Lineage: mat1._c4 EXPRESSION [(mat1)default.mat1.FieldSchema(name:_c4, type:int, comment:null), (t1)t1.FieldSchema(name:d, type:int, comment:null), ]
POSTHOOK: Lineage: mat1.a SIMPLE [(t1)t1.FieldSchema(name:a, type:int, comment:null), ]
POSTHOOK: Lineage: mat1.b SIMPLE [(t1)t1.FieldSchema(name:b, type:varchar(256), comment:null), ]
$hdt$_0.row__id	t1.a	t1.b	_c2	_c3	_c4
PREHOOK: query: EXPLAIN CBO
SELECT a, b, sum(d), min(d), max(d)
FROM t1
WHERE c > 10.0
GROUP BY a, b
ORDER BY a, b
PREHOOK: type: QUERY
PREHOOK: Input: default@mat1
PREHOOK: Input: default@t1
#### A masked pattern was here ####
POSTHOOK: query: EXPLAIN CBO
SELECT a, b, sum(d), min(d), max(d)
FROM t1
WHERE c > 10.0
GROUP BY a, b
ORDER BY a, b
POSTHOOK: type: QUERY
POSTHOOK: Input: default@mat1
POSTHOOK: Input: default@t1
#### A masked pattern was here ####
Explain
CBO PLAN:
HiveSortLimit(sort0=[$0], sort1=[$1], dir0=[ASC], dir1=[ASC])
  HiveProject(a=[$0], b=[$1], _c2=[$2], _c3=[$3], _c4=[$4])
    HiveTableScan(table=[[default, mat1]], table:alias=[default.mat1])

PREHOOK: query: SELECT a, b, sum(d), min(d), max(d)
FROM t1
WHERE c > 10.0
GROUP BY a, b
ORDER BY a, b
PREHOOK: type: QUERY
PREHOOK: Input: default@mat1
PREHOOK: Input: default@t1
#### A masked pattern was here ####
POSTHOOK: query: SELECT a, b, sum(d), min(d), max(d)
FROM t1
WHERE c > 10.0
GROUP BY a, b
ORDER BY a, b
POSTHOOK: type: QUERY
POSTHOOK: Input: default@mat1
POSTHOOK: Input: default@t1
#### A masked pattern was here ####
a	b	_c2	_c3	_c4
1	new row with NULL aggregated value	NULL	NULL	NULL
1	row with NULL aggregated value	NULL	NULL	NULL
1	row with NULL aggregated value will be inserted	NULL	NULL	NULL
2	average row	21	10	11
2	new average row	101	50	51
3	average row will be inserted	40	20	20
4	row with NULL aggregated value + non null value will be inserted	100	100	100
5	average row + null value will be inserted	11	11	11
NULL	new row with NULL GBY key	71	35	36
NULL	row with NULL GBY key	15	7	8
NULL	row with NULL GBY key will be inserted	14	7	7
NULL	NULL	200	100	100
PREHOOK: query: SELECT * FROM mat1
ORDER BY a, b
PREHOOK: type: QUERY
PREHOOK: Input: default@mat1
#### A masked pattern was here ####
POSTHOOK: query: SELECT * FROM mat1
ORDER BY a, b
POSTHOOK: type: QUERY
POSTHOOK: Input: default@mat1
#### A masked pattern was here ####
mat1.a	mat1.b	mat1._c2	mat1._c3	mat1._c4
1	new row with NULL aggregated value	NULL	NULL	NULL
1	row with NULL aggregated value	NULL	NULL	NULL
1	row with NULL aggregated value will be inserted	NULL	NULL	NULL
2	average row	21	10	11
2	new average row	101	50	51
3	average row will be inserted	40	20	20
4	row with NULL aggregated value + non null value will be inserted	100	100	100
5	average row + null value will be inserted	11	11	11
NULL	new row with NULL GBY key	71	35	36
NULL	row with NULL GBY key	15	7	8
NULL	row with NULL GBY key will be inserted	14	7	7
NULL	NULL	200	100	100
PREHOOK: query: DROP MATERIALIZED VIEW mat1
PREHOOK: type: DROP_MATERIALIZED_VIEW
PREHOOK: Input: default@mat1
PREHOOK: Output: default@mat1
POSTHOOK: query: DROP MATERIALIZED VIEW mat1
POSTHOOK: type: DROP_MATERIALIZED_VIEW
POSTHOOK: Input: default@mat1
POSTHOOK: Output: default@mat1
PREHOOK: query: EXPLAIN CBO
SELECT a, b, sum(d), min(d), max(d)
FROM t1
WHERE c > 10.0
GROUP BY a, b
ORDER BY a, b
PREHOOK: type: QUERY
PREHOOK: Input: default@t1
#### A masked pattern was here ####
POSTHOOK: query: EXPLAIN CBO
SELECT a, b, sum(d), min(d), max(d)
FROM t1
WHERE c > 10.0
GROUP BY a, b
ORDER BY a, b
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1
#### A masked pattern was here ####
Explain
CBO PLAN:
HiveSortLimit(sort0=[$0], sort1=[$1], dir0=[ASC], dir1=[ASC])
  HiveProject(a=[$0], b=[$1], $f2=[$2], $f3=[$3], $f4=[$4])
    HiveAggregate(group=[{0, 1}], agg#0=[sum($3)], agg#1=[min($3)], agg#2=[max($3)])
      HiveFilter(condition=[>($2, 10:DECIMAL(2, 0))])
        HiveTableScan(table=[[default, t1]], table:alias=[t1])

PREHOOK: query: SELECT a, b, sum(d), min(d), max(d)
FROM t1
WHERE c > 10.0
GROUP BY a, b
ORDER BY a, b
PREHOOK: type: QUERY
PREHOOK: Input: default@t1
#### A masked pattern was here ####
POSTHOOK: query: SELECT a, b, sum(d), min(d), max(d)
FROM t1
WHERE c > 10.0
GROUP BY a, b
ORDER BY a, b
POSTHOOK: type: QUERY
POSTHOOK: Input: default@t1
#### A masked pattern was here ####
a	b	_c2	_c3	_c4
1	new row with NULL aggregated value	NULL	NULL	NULL
1	row with NULL aggregated value	NULL	NULL	NULL
1	row with NULL aggregated value will be inserted	NULL	NULL	NULL
2	average row	21	10	11
2	new average row	101	50	51
3	average row will be inserted	40	20	20
4	row with NULL aggregated value + non null value will be inserted	100	100	100
5	average row + null value will be inserted	11	11	11
NULL	new row with NULL GBY key	71	35	36
NULL	row with NULL GBY key	15	7	8
NULL	row with NULL GBY key will be inserted	14	7	7
NULL	NULL	200	100	100
